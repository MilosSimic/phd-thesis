%!TEX root =  main.tex
\chapter{Research review}\label{chapter:Review}
%
In this chapter, we present the the results of the research reviews addressing issues and limits of CC discussed earlier are presented -- both academia, and the industry researching and developing viable solutions to help cloud in the future. 

Few directions are feasible: \textbf{(1)} focusing on adapting existing solutions to fit EC model, \textbf{(2)} experiment and develop new ideas and solutions to maybe fit more the nature of EC, and \textbf{(3)} try to combine both ideas to cover more research area.

Existing nodes organizational abilities are reviewed in section~\ref{sec:nodes_organization}. Section~\ref{sec:nodes_organization} reviews platform models from both industry and academia. Section~\ref{sec:task_offloading} reviews cloud offloading techniques, whereas section~\ref{sec:applications} reviews some application models. Section~\ref{sec:infrastructure_management} reviews some models used in cloud infrastructure management. Section~\ref{sec:thesis_position} concludes this chapter, and gives the position of this thesis, compared to research previously reviewed.
%
%
\section{Nodes organization}\label{sec:nodes_organization}
Guo et al.~\cite{GuoRG20}, gives a promising model based on a zone-based organization of edge nodes in the smart vehicles application. The authors show how zone-based nodes organization enables continuity of dynamic services and reduces the connection handovers. They prove it is possible to enlarge the coverage of edge servers to a bigger zone, but at the same time, the computing power and storage capacity of edge servers could be expanded. EC could be based on the geo-distributed workloads, and this zone-based organization could benefit the EC model in various ways. 

In their research~\cite{BaktirOE17}, Baktir et al. explored the capabilities of software-defined networks (SDN) from a programming standpoint. Their findings show that SDN can be used to simplify the management of the network in a cloud-like environment. Networking in such a complex environment is not an easy task to achieve. The authors show how SDN hides the complexity of the heterogeneous environment from the end-users. As such, SDNs represent a good candidate for networking tasks in complex, cloud-like environments. In~\cite{abs-2106-10014} Sherwin et al. investigate the programmatic control over the resources and functions of a network to make the network more dynamically configurable.

Sayed et al. in their work~\cite{El-SayedSPPGML18} show that EC systems will perform actions before connecting to the cloud and how they are easier to integrate with other wireless networks like mobile ad-hoc networks (MANETs), vehicular ad-hoc networks (VANETs), intelligent transport systems (ITSs) and the IoT to mitigate network-related and computational problems.

Content delivery networks (CDN) in centralized delivery models like CC have bad scalability, as Kurniawan et al.~\cite{inbookKurniawan} argue in their research. To overcome these centralized problems and bad scalability, the authors proposed a different solution, a decentralized solution. To achieve such tasks, authors were using a network of gateways equipped with some storage as well, for internet services at home~\cite{inbookKurniawan} forming even smaller DCs -- nano DCs ($n$DCs). Authors present a possible usage for these $n$DCs in some large scale applications with much less energy consumption than traditional DCs.

In their paper~\cite{CiobanuNPDMM19}, Ciobanu et al. introduce an interesting idea called \textit{drop computing}. The authors show the possibility for ad-hoc EC platform composition using a decentralized model over multilayered social crowd networks. This idea gives us the ability for collaborative computing that can be formed dynamically. The authors present an idea that we can form a computing group ad-hoc by employing the nearby devices in the mobile crowd, that is fully capable of quick and efficient access to resources, instead of sending requests to the cloud. Crowd nodes could also be an interesting idea for backup nodes, in case more computing power or storage is needed and there no more available resources to use. Forming platforms from crowd resources and ad-hoc, raises a few concerns: \textbf{(1)} crowd nodes availability, and \textbf{(2)} offered resources. 

Greenberg et al.~\cite{GreenbergHMP09} introduce the idea of $\upmu$DCs as DCs that operate in proximity to a big population compared to $n$DCs that serve a lot smaller population, for example, a single household. $\upmu$DCs are an interesting model and area of rapid innovation and development, and because they are close to some population, they are minimizing the costs and the latency for end-users~\cite{GreenbergHMP09}, Their minimum size is defined by the needs of the local population~\cite{GreenbergHMP09, AbbasZTS18}, as such, they are reducing traditional DCs fixed costs. The main feature that $\upmu$DCs are relying on is agility. The authors describe agility as $\upmu$DCs ability to dynamically grow and shrink resources to satisfy the resource demands and usage from the most optimal location~\cite{GreenbergHMP09}. In~\cite{ShaoLFJL19} Shao et al. present a possible $\upmu$DCs structure serving only the local population, in the smart city use-case.
%
%
\section{Platform models}\label{sec:platform_models}
%
Kubernetes (k8s for short)~\cite{BurnsGOBW16} is a system originally developed by Google influenced by their orchestrator platform called Borg~\cite{VermaPKOTW15}. Various other companies joined in developing this system, and now it is de facto standard in the cloud environment for microservices and cloud-native applications. By design, Kubernetes is not intended to operate in a geo-distributed environment, because it operates on a single cluster~\cite{BurnsGOBW16, VermaPKOTW15, RossiCPN20}. As such it might not be best suited for EC and geo-distributed $\upmu$Cs. Nonetheless, it is a super valuable tool in the CC because it enables health checking, restarting, and orchestration at scale. Another potential problem with Kubernetes is its relatively complicated deployment concept that might be too complicated for EC workloads. It is developed to connect microservices across the CC environment. It could be used as it is, a cloud-native orchestrator to run the master process for EC and $\upmu$Cs and also cloud-native applications that will accept streams coming from $\upmu$C applications. Kubernetes relies upon a lot to work properly. Existing technologies and ideas worth exploring, such as loosely coupling elements with labels and selectors, should not be ignored.

In their research, Rossi et al.~\cite{RossiCPN20} present a solution based on Kubernetes. Authors adapted Kubernetes for workloads that are geo-distributed and they had used reinforcement learning (RL) techniques, to learn a suitable scaling policy from past experience. There are potential downsides to this approach. The first might be that machine learning implementation could be potentially slow due to the required model training, but also we need to somehow describe what a good decision, and what a bad decision is, in order to enable some algorithm to learn this. This might be a problematic thing, especially in urgent situations. The second potential problem is that, even though Kubernetes is a promising solution and de-facto standard in the CC environment, as previously stated, it might not be the best proposal for EC and geo-distributed $\upmu$C environment. Despite all these potential problems, researchers show great work in adapting Kubernetes architecture to work for geo-distributed workloads.

Ryden et al.~\cite{RydenOCW14} present an interesting platform for distributed computing, that is more oriented towards user-based applications\label{sec:rayden}. Compared to other similar systems. their goal was not to develop a solution that will do resource management policy, on the contrary, their focus is more oriented to give flexibility to the users for application development. Users can develop their applications using exclusively Javascript programming language, with some embedded native code for a more efficient solution. The authors rely on a bunch of volunteer nodes to run all the tasks and applications, similar to work presented in~\cite{CiobanuNPDMM19}. The main difference between these two solutions is that Ryden et al. make a split on which nodes are storage nodes, and which nodes are used for calculation and processing tasks. Their application environment is protected from malicious code, using sandboxing techniques. This presents an interesting work to show how users can develop applications and run them in an EC environment.

In~\cite{LebrePSD17} L{\`{e}}bre et al. show an interesting solution based on extending and adopting the OpenStack system. OpenStack is a free and open standard cloud computing IaaS platform for CC use cases in both public and private clouds. The authors tried to manage both cloud and edge resources using a NoSQL database. Massively distributed multi-site IaaS, using OpenStack is a challenging task~\cite{LebrePSD17} to implement, because the communication between nodes of different sites can be subject to important network latencies~\cite{LebrePSD17}. On the other hand, if it could be done properly we would gain one major advantage that users of the IaaS solution can continue using the same familiar infrastructure for both cloud and edge/fog use-cases.

Based on the literature survey, the Ning et al. presents current open issues of EC platforms~\cite{NingLSY20}. In their work, authors focus on different aspects of EC systems, and they outline the importance of EC and CC tight collaboration. The CC needs to be unloaded and EC nodes could provide data pre-processing. On the other hand, EC needs massive storage and a strong computing capacity of CC. The authors illustrate the usage of edge computing platforms to build specific applications and conclude that with CC and EC integration, both sides will benefit.

In~\cite{abs-1802-10375} the de Guzm{\'{a}}n et al. present solution based on Kubernetes that use Kubernetes Deployment Manifests to reuse successful principles from Kubernetes by creating a virtual machine for each Pod using Linuxkit. Their solution is based on the immutable infrastructure pattern, and instead of containers, they use the virtual machines as the unit of deployment. Authors prove that the attack surface of their system is reduced since Linuxkit only installs the minimum OS dependencies to run containers. It represents interesting usage of LinuxKit to deploy OS dependencies and immutable infrastructure patterns, but VMs might be a bit problem for small devices, and ARM nodes as well as the complex flow of the Kubernetes application model. Nonetheless, it is an interesting extension of the Kubernetes framework and proves that LinuxKit can be used for immutable infrastructures with custom OS.

In~\cite{SamiM20} Sami et al. show an interesting platform for dynamic services distribution over Fog nodes using volunteer nodes. Their platform is tuned for container placement with relevance and efficiency on volunteering fog devices, near users with maximum time availability and shortest distance. They do this \textit{on the fly}  with improved QoS.

Besides academy efforts, the industry as well introduced a few interesting platforms and frameworks for EC. For example, Amazon introduced their framework Greengrass~\cite{kurniawan_2018} that can run on various hardware to do some processing. Amazon turns to the option that their framework is deeply connected to the rest of the AWS cloud ecosystem.  KubeEdge~\cite{KubeEdge} is a lightweight extension of the Kubernetes framework, to operate in an EC environment. The same as regular Kubernetes, all workloads are done in the domain of a single cluster which might not be the best solution for geo-distributed $\upmu$Cs. Both Greengrass and KubeEdge are frameworks that are mainly used for user-based applications. On the other hand, there is General Electric with its Predix~\cite{GE_Predix} platform. Predix is a scalable platform used for industrial IoT applications.

Osmotic computing is a relatively new infrastructure paradigm aiming to decompose applications into services dynamically tailored for the smart environments exploiting resources in edge and cloud infrastructures~\cite{VillariFDRJR19}. The applications developed for this computing paradigm are deployed in the cloud and edge systems equalizing the microservices concentrations on both sides. This thesis can serve as a base for osmotic computing, allowing dynamic and efficient management of $\upmu$C infrastructure to avoid application breakdown and degradation of QoS.
%
%
\section{Task offloading}\label{sec:task_offloading}
%
As already mentioned in~\ref{sec:mobile_computing} EC nodes rely on the concept of data and computation offloading from the cloud closer to the ground \cite{KhuneP19}, while heavy computation remains in the cloud because of resource availability~\cite{NingLSY20}. 

Offloading is an effective strategy when using cloud services. Relying only on cloud services may be prone to introducing long latency, which some applications cannot tolerate. On the other hand, mobile devices and sensors do not have sufficient battery energy for task offloading~\cite{MaoZL16}. The computation performance may be compromised due to insufficient battery energy for task offloading, so these devices might send their data to nearby EC nodes.

In literature, there are few platforms proposing task offloading~\cite{ShiHPANZ14, KhuneP19, ChenHLLW15, LinLJL19, JiangCGZW19, MaoZL16} to the nearby edge layer. These offloading techniques are based on different parameters, options, and techniques to put tasks to different sets of nodes in such a way that it won't drain mobile devices and sensors battery. After the computation is done, this edge layer sends pre-processed data to the cloud for further analysis, storage, etc.

When using task offloading techniques, it is very important to have good QoS that users can rely on. In~\cite{SamiM20} authors used Evolutionary Memetic Algorithm (MA) to solve their multi-objective container placement optimization problem to achieve better QoS.

One of the key challenges in the area of computation offloading is in the mismatch between how devices demand and access computing resources and how cloud providers offer them~\cite{ShiHPANZ14}. For example, it takes around 27 seconds to start single VM instance on the AWS, but the leasing time for the single VM instance is one hour. On the other hand, execution delay is stable in the CC, while in the EC is not due to the computational and transmission delay~\cite{WangZMHNW18}.
%
%
\section{Application models}\label{sec:applications}
%
Sayed et al. in their work~\cite{El-SayedSPPGML18} describe that EC follows a decentralized architecture model and that data processing is at the edge of the network, thus it enables nodes to make autonomous decisions.  So the applications written for EC can perform actions locally before connecting to the cloud at all. This will have some benefits like reducing network overhead issues as well as the security and privacy issues. 

As already mentioned on page~\pageref{sec:rayden}, Ryden et al.~\cite{RydenOCW14} present an interesting user-oriented platform for distributed computing called Nebula. In this section, their research is going to be dissected, but from a different angle. Nebula allows users to develop their applications using JS exclusively, due to the usage of Google Chrome Web browser-based Native Client (NaCl) sandbox~\cite{YeeSDCMOONF10} that can run JS code only. Restriction on a single language might be a problem for some users and use-cases even though JS is a popular language at the moment. On the other hand, virtual machines tend to be too resource-demanding packing stuff that might not be needed, so a solution using containers or unikernels might provide better resource utilization and pack more services per node than virtual machines.

In~\cite{SatyanarayananBCD09} Satyanarayanan et al. represent an interesting view on cloudlets as a \say{data center in a box.}. They give an example that cloudlets should support a wide range of users, with minimal constraints on their software. They put emphasis on transient VM technology. The emphasis on transient VMs is because cloudlet infrastructure is restored to its pristine software state after each use, without manual intervention. At the time when they conducted their research, containers might not have been working solution or it might have been hard to use them. By modern standards, containers may even fit better, and pack more user software on the same hardware. This may be the case for the unikernels, once they reach a wider adoption rate and stable products.

Various Kubernetes variants like~\cite{KubeEdge, RossiCPN20}, give users the possibility to run different applications like web servers and databases even on smaller devices creating green DC~\cite{ArocaG12}.

Satyanarayanan et al.~\cite{SatyanarayananK19} propose the concept of edge-native applications that will separate space into 3 layers or tiers. Tier \textbf{(1)} represents various mobile, IoT devices autonomous vehicles, etc, and these devices produce a lot of data. Tier \textbf{(2)} represents applications running in cloudlets or other EC models, that will be able to do some pre-processing, or data filtering before it goes further. Finally, tier \textbf{(3)} represents classic cloud applications that will accept pre-processed and filtered data from the previous tier, do more processing, react on some values, or store for future use. This idea represents an interesting concept and gives wide space for users and application development.

In~\cite{inproceedingsBeck} Beck et al. argue that applications should use message bus, streams, or topics because most mobile or edge applications are expected to be event-driven. The message bus system is an interesting proposition because the virtualized applications can subscribe to message streams, i.e., topics, and act only when data arrive. Applications might not be alive the whole time. And if for some reason mobile edge applications cannot reach a close EC server, it can always send data to the cloud. So cloud applications should be changed so slightly, just to cover this edge case.

In~\cite{JararwehDAAAB16}, Jararweh et al. show how integration between EC with CC principles will create more complex services and applications at the edge of the network opening new possibilities for applications to reduce the load on the centralized cloud model but also avoid bottlenecks and single points of failure.

In~\cite{cherrueau}, authors propose solution that relies on the modularity of the microservices: \textbf{(1)} one application instance is at the edge, making the system robust to network partitions (local requests can still be satisfied), and \textbf{(2)} collaboration can be programmed with service mesh.
%
%
\section{Infrastructure management}\label{sec:infrastructure_management}
%
In the era of distributed systems, cloud computing, and microservices the open-source community and different companies provided various tools for the purpose of abstracting infrastructure at the level of software. These tools can be separated into two subgroups, based on how users send instructions to the systems on \textbf{(1)} declarative, and \textbf{(2)} imperative.

Newly created tools like Terraform, Polumni, or CFEngine are representative of the declarative movement, using platform-independent language to specify configuration, policies, security, and much more. Users specify their intentions usually through some existing formats like \emph{JSON} or \emph{YAML}, while others use their domain-specific language to achieve the same goal.

In both cases, users do not specify explicit commands that the system needs to execute. Instead, they declare what they want to achieve, and leave it for the system to determine the optimal way of achieving it. The main difference between an existing format, and a domain-specific language is how verbose specification is. Existing formats may be a little bit more verbose, but users are most likely already familiar with them. Developing domain-specific language takes more time and requires users to learn them from scratch, but the final outcome is usually a more concise, expressive and optimized set of instructions.

These tools are turning out to be very important in a multi-cloud environment. The users need to specify artifacts, independently from the cloud provider, and let the system deal with the cloud provider specifics.

On the other hand, already existing and well-known tools like Chef, Ansible, and Puppet usually rely on some specific language, and the user needs to code the instructions that must be done to achieve the same or similar job.

This is more prone to error, since users may introduce a bug in the system that might be hard to debug and find. At the same time, these tools have existed for a long time, and there are existing best practices and a lot of available examples for users to utilize.

Every major cloud provider offers a proprietary solution, deeply integrated into their ecosystem. AWS offers CloudFormation, a configuration tool that allows users to code their infrastructure to automate deployments.

Microsoft Azure offers a Resource Manager that allows users to define the infrastructure and dependencies in templates, and organize dependent resources into groups that can be deployed or deleted in a single action, access control, and more.

Google Cloud offers a Deployment Manager that offers many features to automate cloud infrastructure stack. Users can create templates using \emph{YAML} or \emph{Python} programming language. They can preview changes before deployment, in a console user interface, and much more.

In terms of adoptability in $\upmu$DCs on the edge, already existing tools might be used to set up infrastructure in both places, in the cloud and at the edge, if they allow extensibility. Declarative tools would be harder to adopt since they are built with different goals in mind. While imperative tools might be easier to utilize, they may introduce unnecessary complications to the system.

On the other hand, a newly created IaC system can communicate with existing tools via some standard interfaces to set up the cloud infrastructure, while a new, custom made engine could handle organizing geo-distributed nodes into $\upmu$DCs.

With this strategy, we are getting the best of both worlds. The new user application model would be based on the existing cloud application model, offering users a fast and elegant way to develop new human-centered applications.

IaC has grown in popularity in recent years because it applies the same kind of version control and repeatability to the orchestration of the infrastructure as developers use for applications source code~\cite{ArtacBNGT17}. The second benefit is that configuration is decoupled from the system, it can more readily be deployed on a similar system elsewhere.
%
%
\section{Thesis position}\label{sec:thesis_position}
%
In the previous sections, different aspects of EC and integration with the CC, influential research, interesting concepts, and implementations were described.

The focus of this thesis is to make the connection between CC and EC stronger, represented as a system that can descriptively and dynamically organize geo-distributed nodes that already exist over an arbitrary vast area into one coherent system --- $\upmu$Cs, that could be offered to users as a service. This approach is not fully addressed in other solutions. 

This thesis is influenced by the organization of CC and their big DCs but adapted for a different environment such as EC and $\upmu$Cs, influenced by the work described in previous sections. 

Adaptations that are required for such tasks must be followed by a clear Separation of concerns (SoC) model and intuitive applications model so that users can fully use new-formed infrastructure properly. 

All these adaptations made to traditional CC model will make it possible to push the whole solution more towards EC as a service and $\upmu$C model that can help CC with latency issues with new-age applications.
%
%